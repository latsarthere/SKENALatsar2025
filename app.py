import streamlit as st

import pandas as pd

import time

import io

import requests

from bs4 import BeautifulSoup

from datetime import date, datetime, timedelta

from pygooglenews import GoogleNews



# --- Konfigurasi Halaman Streamlit ---

st.set_page_config(

Â  Â  page_title="SKENA",

Â  Â  page_icon="logo skena.png",

Â  Â  layout="wide"

)



# --- TEMA WARNA & GAYA ---

custom_css = """

<style>

Â  Â  .block-container { padding-top: 2rem; }

Â  Â  h1, h2, h3, h4, h5 { color: #0073C4; }

Â  Â  div[data-testid="stButton"] > button[kind="primary"],

Â  Â  div[data-testid="stForm"] > form > div > button {

Â  Â  Â  Â  background-color: #0073C4; color: white; border: none;

Â  Â  }

Â  Â  div[data-testid="stButton"] > button[kind="primary"]:hover,

Â  Â  div[data-testid="stForm"] > form > div > button:hover {

Â  Â  Â  Â  background-color: #005A9E; color: white;

Â  Â  }

Â  Â  [data-testid="stSidebar"] { background-color: #f8f9fa; }

Â  Â  .stAlert { border-radius: 5px; }

Â  Â  .stAlert[data-baseweb="notification"][data-testid*="info"] { border-left: 5px solid #0073C4; }

Â  Â  .stAlert[data-baseweb="notification"][data-testid*="success"] { border-left: 5px solid #65B32E; }

Â  Â  .stAlert[data-baseweb="notification"][data-testid*="warning"] { border-left: 5px solid #F17822; }

Â  Â Â 

Â  Â  /* [BARU] Class untuk perataan teks */

Â  Â  .text-center { text-align: center; }

Â  Â  .text-justify { text-align: justify; }

</style>

"""

st.markdown(custom_css, unsafe_allow_html=True)





# --- FUNGSI-FUNGSI PENDUKUNG ---

@st.cache_data

def load_data_from_url(url, sheet_name=0):

Â  Â  try:

Â  Â  Â  Â  df = pd.read_excel(url, sheet_name=sheet_name)

Â  Â  Â  Â  return df

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Gagal memuat data dari URL: {e}")

Â  Â  Â  Â  return None



def get_rentang_tanggal(tahun: int, triwulan: str, start_date=None, end_date=None):

Â  Â  if triwulan == "Tanggal Custom":

Â  Â  Â  Â  if start_date and end_date:

Â  Â  Â  Â  Â  Â  return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')

Â  Â  Â  Â  return None, None

Â  Â  triwulan_map = {

Â  Â  Â  Â  "Triwulan 1": (f"{tahun}-01-01", f"{tahun}-03-31"),

Â  Â  Â  Â  "Triwulan 2": (f"{tahun}-04-01", f"{tahun}-06-30"),

Â  Â  Â  Â  "Triwulan 3": (f"{tahun}-07-01", f"{tahun}-09-30"),

Â  Â  Â  Â  "Triwulan 4": (f"{tahun}-10-01", f"{tahun}-12-31"),

Â  Â  }

Â  Â  return triwulan_map.get(triwulan, (None, None))



def ambil_ringkasan(link):

Â  Â  try:

Â  Â  Â  Â  headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

Â  Â  Â  Â  response = requests.get(link, timeout=10, headers=headers)

Â  Â  Â  Â  response.raise_for_status()

Â  Â  Â  Â  soup = BeautifulSoup(response.text, 'html.parser')

Â  Â  Â  Â  deskripsi = soup.find('meta', attrs={'name': 'description'})

Â  Â  Â  Â  if deskripsi and deskripsi.get('content'): return deskripsi['content']

Â  Â  Â  Â  og_desc = soup.find('meta', attrs={'property': 'og:description'})

Â  Â  Â  Â  if og_desc and og_desc.get('content'): return og_desc['content']

Â  Â  Â  Â  p_tag = soup.find('p')

Â  Â  Â  Â  if p_tag: return p_tag.get_text(strip=True)

Â  Â  except Exception:

Â  Â  Â  Â  return ""

Â  Â  return ""



def start_scraping(tanggal_awal, tanggal_akhir, kata_kunci_lapus_df, kata_kunci_daerah_df, start_time, table_placeholder, keyword_placeholder):

Â  Â  kata_kunci_lapus_dict = {c: kata_kunci_lapus_df[c].dropna().astype(str).str.strip().tolist() for c in kata_kunci_lapus_df.columns}

Â  Â  nama_daerah = "Konawe Selatan"

Â  Â Â 

Â  Â  kecamatan_list = kata_kunci_daerah_df[nama_daerah].dropna().astype(str).str.strip().tolist()

Â  Â  lokasi_filter = [nama_daerah.lower()] + [kec.lower() for kec in kecamatan_list]

Â  Â Â 

Â  Â  status_placeholder = st.empty()

Â  Â  gn = GoogleNews(lang='id', country='ID')

Â  Â Â 

Â  Â  if 'hasil_scraping' not in st.session_state:

Â  Â  Â  Â  st.session_state.hasil_scraping = []

Â  Â Â 

Â  Â  for kategori, kata_kunci_list in kata_kunci_lapus_dict.items():

Â  Â  Â  Â  for keyword_raw in kata_kunci_list:

Â  Â  Â  Â  Â  Â  elapsed_time = time.time() - start_time

Â  Â  Â  Â  Â  Â  status_placeholder.info(f"â³ Memproses: {kategori} | Waktu: {int(elapsed_time // 60)}m {int(elapsed_time % 60)}d")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if pd.isna(keyword_raw): continue

Â  Â  Â  Â  Â  Â  keyword = str(keyword_raw).strip()

Â  Â  Â  Â  Â  Â  if not keyword: continue

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  keyword_placeholder.text(f"Â  â¡ï¸ ğŸ” Mencari: '{keyword}'")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  search_query = f'"{keyword}" "{nama_daerah}"'

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  search_results = gn.search(search_query, from_=tanggal_awal, to_=tanggal_akhir)

Â  Â  Â  Â  Â  Â  Â  Â  for entry in search_results['entries']:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  link = entry.link

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if any(d['Link'] == link for d in st.session_state.hasil_scraping): continue



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  judul = entry.title

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ringkasan = ambil_ringkasan(link)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  judul_lower, ringkasan_lower, keyword_lower = judul.lower(), ringkasan.lower(), keyword.lower()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lokasi_ditemukan = any(loc in judul_lower or loc in ringkasan_lower for loc in lokasi_filter)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keyword_ditemukan = keyword_lower in judul_lower or keyword_lower in ringkasan_lower



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if lokasi_ditemukan or keyword_ditemukan:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tanggal_dt = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %Z')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tanggal_str = tanggal_dt.strftime('%d-%m-%Y')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except (ValueError, TypeError):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tanggal_str = "N/A"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.hasil_scraping.append({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Nomor": len(st.session_state.hasil_scraping) + 1,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Kategori": kategori,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Kata Kunci": keyword, "Judul": judul,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Link": link, "Tanggal": tanggal_str, "Ringkasan": ringkasan

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  continue



Â  Â  Â  Â  if st.session_state.hasil_scraping:

Â  Â  Â  Â  Â  Â  df_live = pd.DataFrame(st.session_state.hasil_scraping)

Â  Â  Â  Â  Â  Â  kolom_urut = ["Nomor", "Kategori", "Kata Kunci", "Judul", "Link", "Tanggal", "Ringkasan"]

Â  Â  Â  Â  Â  Â  df_live = df_live[kolom_urut]

Â  Â  Â  Â  Â  Â  with table_placeholder.container():

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("### Hasil Scraping Terkini")

Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(df_live, use_container_width=True, height=400)

Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"Total berita ditemukan: {len(df_live)}")



Â  Â  status_placeholder.empty()

Â  Â  keyword_placeholder.empty()

Â  Â Â 

Â  Â  if st.session_state.hasil_scraping:

Â  Â  Â  Â  return pd.DataFrame(st.session_state.hasil_scraping)

Â  Â  else:

Â  Â  Â  Â  return pd.DataFrame()



# --- HALAMAN-HALAMAN APLIKASI ---



def set_page(page_name):

Â  Â  st.session_state.page = page_name



def show_home_page():

Â  Â  st.image("logo skena full.png", use_container_width=True)

Â  Â  st.markdown("---")

Â  Â Â 

Â  Â  # --- [DIUBAH] Teks pengantar menjadi justify ---

Â  Â  st.markdown("""

Â  Â  <div class='text-justify'>

Â  Â  Â  Â  Hallo! Sistem Scraping Konawe Selatan (SKENA) merupakan alat bantu BPS Kabupaten Konawe Selatan dalam menyediakan data statistik yang lengkap.Â 

Â  Â  Â  Â  Sistem ini melakukan pencarian (<i>scraping</i>) fenomena pendukung dalam bentuk berita di Google.

Â  Â  </div>

Â  Â  """, unsafe_allow_html=True)



Â  Â  # --- [DIUBAH] Tombol Pendahuluan menjadi justify ---

Â  Â  st.markdown("""

Â  Â  <div class='text-justify' style='margin-top: 10px;'>

Â  Â  Â  Â  Sebelum mengakses fitur utama, sangat disarankan untuk membaca bagian <b>Pendahuluan</b> terlebih dahulu.

Â  Â  </div>

Â  Â  """, unsafe_allow_html=True)

Â  Â Â 

Â  Â  if not st.session_state.get('logged_in', False):

Â  Â  Â  Â  # --- [DIUBAH] Info Login menjadi justify ---

Â  Â  Â  Â  st.markdown("<div class='text-justify' style='margin-top: 1rem;'>", unsafe_allow_html=True)

Â  Â  Â  Â  st.info("Silakan Login melalui sidebar untuk menggunakan menu Scraping dan Dokumentasi.")

Â  Â  Â  Â  st.markdown("</div>", unsafe_allow_html=True)

Â  Â Â 

Â  Â  st.markdown("<h2 class='text-center' style='margin-top: 2rem;'>Pilih Kategori Data</h2>", unsafe_allow_html=True)

Â  Â Â 

Â  Â  col1_btn, col2_btn, col3_btn, col4_btn = st.columns(4, gap="large")

Â  Â  is_disabled = not st.session_state.get('logged_in', False)

Â  Â Â 

Â  Â  with col1_btn:

Â  Â  Â  Â  st.subheader("ğŸ“ˆ Neraca")

Â  Â  Â  Â  if st.button("Pilih Neraca", key="home_neraca", use_container_width=True, disabled=is_disabled):

Â  Â  Â  Â  Â  Â  st.session_state.page = "Scraping"; st.session_state.sub_page = "Neraca"; st.rerun()

Â  Â  with col2_btn:

Â  Â  Â  Â  st.subheader("ğŸ‘¥ Sosial")

Â  Â  Â  Â  if st.button("Pilih Sosial", key="home_sosial", use_container_width=True, disabled=is_disabled):

Â  Â  Â  Â  Â  Â  st.session_state.page = "Scraping"; st.session_state.sub_page = "Sosial"; st.rerun()

Â  Â  with col3_btn:

Â  Â  Â  Â  st.subheader("ğŸŒ¾ Produksi")

Â  Â  Â  Â  if st.button("Pilih Produksi", key="home_produksi", use_container_width=True, disabled=is_disabled):

Â  Â  Â  Â  Â  Â  st.session_state.page = "Scraping"; st.session_state.sub_page = "Produksi"; st.rerun()

Â  Â  with col4_btn:

Â  Â  Â  Â  st.subheader("ğŸ“‘ Lainnya")

Â  Â  Â  Â  if st.button("Pilih Lainnya", key="home_lainnya", use_container_width=True, disabled=is_disabled):

Â  Â  Â  Â  Â  Â  st.session_state.page = "Scraping"; st.session_state.sub_page = "Lainnya"; st.rerun()



def show_pendahuluan_page():

Â  Â  st.title("ğŸ“– Pendahuluan")

Â  Â  st.markdown("---")

Â  Â  st.markdown("""

Â  Â  Selamat datang di **SKENA (Sistem Scraping Fenomena Konawe Selatan)**.



Â  Â  Aplikasi ini dirancang untuk membantu dalam pengumpulan data berita online yang relevan dengan Kabupaten Konawe Selatan.Â 

Â  Â  Dengan memanfaatkan teknologi web scraping, SKENA dapat secara otomatis mencari, mengumpulkan, dan menyajikan data dari berbagai sumber berita di internet.

Â  Â  """)

Â  Â  if not st.session_state.get('logged_in', False):

Â  Â  Â  Â  st.markdown("Silakan **Login** melalui sidebar untuk mengakses fitur utama.")



def show_documentation_page():

Â  Â  st.title("ğŸ—‚ï¸ Dokumentasi")

Â  Â  st.markdown("Seluruh file, dataset, dan dokumentasi terkait proyek ini tersimpan di Google Drive.")

Â  Â Â 

Â  Â  folder_id = "1z1_w_FyFmNB7ExfVzFVc3jH5InWmQSvZ"

Â  Â  folder_url = f"https://drive.google.com/drive/folders/{folder_id}"

Â  Â  st.link_button("Buka Google Drive", folder_url, use_container_width=True, type="primary")

Â  Â Â 

Â  Â  st.markdown("---")

Â  Â Â 

Â  Â  with st.expander("Tampilkan Pratinjau Folder di Sini"):

Â  Â  Â  Â  embed_url = f"https://drive.google.com/embeddedfolderview?id={folder_id}"

Â  Â  Â  Â  st.components.v1.html(f'<iframe src="{embed_url}" width="100%" height="600" style="border:1px solid #ddd; border-radius: 8px;"></iframe>', height=620)



def show_scraping_page():

Â  Â  st.title(f"âš™ï¸ Halaman Scraping Data")

Â  Â Â 

Â  Â  sub_page_options = ["Neraca", "Sosial", "Produksi", "Lainnya"]

Â  Â  st.session_state.sub_page = st.radio(

Â  Â  Â  Â  "Pilih Kategori Data:",

Â  Â  Â  Â  sub_page_options,

Â  Â  Â  Â  horizontal=True,

Â  Â  Â  Â  key="sub_page_radio"

Â  Â  )

Â  Â  st.markdown("---")

Â  Â Â 

Â  Â  if st.session_state.sub_page in ["Sosial", "Produksi", "Lainnya"]:

Â  Â  Â  Â  st.header("Segera Hadir!")

Â  Â  Â  Â  st.info(f"Fitur scraping untuk data **{st.session_state.sub_page}** sedang dalam pengembangan.")

Â  Â  Â  Â  st.balloons()

Â  Â  Â  Â  return



Â  Â  url_lapus = "https://docs.google.com/spreadsheets/d/19FRmYvDvjhCGL3vDuOLJF54u7U7hnfic/export?format=xlsx"

Â  Â  url_daerah = "https://docs.google.com/spreadsheets/d/1Y2SbHlWBWwcxCdAhHiIkdQmcmq--NkGk/export?format=xlsx"



Â  Â  with st.spinner("Memuat data kata kunci..."):

Â  Â  Â  Â  df_lapus = load_data_from_url(url_lapus, sheet_name='Sheet1')

Â  Â  Â  Â  df_daerah = load_data_from_url(url_daerah)



Â  Â  if df_lapus is None or df_daerah is None:

Â  Â  Â  Â  st.error("Gagal memuat data kata kunci. Aplikasi tidak dapat berjalan.")

Â  Â  Â  Â  return



Â  Â  st.success("âœ… Data kata kunci berhasil dimuat.")

Â  Â  original_categories = df_lapus.columns.tolist()



Â  Â  st.header("Atur Parameter Scraping")

Â  Â Â 

Â  Â  tahun_sekarang = date.today().year

Â  Â  tahun_list = ["--Pilih Tahun--"] + list(range(2020, tahun_sekarang + 1))

Â  Â  tahun_input = st.selectbox("Pilih Tahun:", options=tahun_list)

Â  Â  triwulan_list = ["--Pilih Triwulan--", "Triwulan 1", "Triwulan 2", "Triwulan 3", "Triwulan 4", "Tanggal Custom"]

Â  Â  triwulan_input = st.selectbox("Pilih Triwulan:", options=triwulan_list)

Â  Â  start_date_input, end_date_input = None, None

Â  Â  if triwulan_input == "Tanggal Custom":

Â  Â  Â  Â  col1, col2 = st.columns(2)

Â  Â  Â  Â  with col1:

Â  Â  Â  Â  Â  Â  start_date_input = st.date_input("Tanggal Awal", date.today() - timedelta(days=30))

Â  Â  Â  Â  with col2:

Â  Â  Â  Â  Â  Â  end_date_input = st.date_input("Tanggal Akhir", date.today())

Â  Â Â 

Â  Â  opsi_kategori_list = ["Semua Kategori", "Pilih Kategori Tertentu"]

Â  Â  mode_kategori = st.radio("Pilih Opsi Kategori:", opsi_kategori_list, horizontal=True)

Â  Â Â 

Â  Â  kategori_terpilih = []

Â  Â  if mode_kategori == 'Pilih Kategori Tertentu':

Â  Â  Â  Â  kategori_terpilih = st.multiselect('Pilih kategori untuk diproses:', options=original_categories)



Â  Â  is_disabled = (tahun_input == "--Pilih Tahun--" or triwulan_input == "--Pilih Triwulan--" or (mode_kategori == 'Pilih Kategori Tertentu' and not kategori_terpilih))



Â  Â  if st.button("ğŸš€ Mulai Scraping", use_container_width=True, type="primary", disabled=is_disabled):

Â  Â  Â  Â  st.session_state.hasil_scraping = []

Â  Â  Â  Â Â 

Â  Â  Â  Â  tahun_int = int(tahun_input)

Â  Â  Â  Â  tanggal_awal, tanggal_akhir = get_rentang_tanggal(tahun_int, triwulan_input, start_date_input, end_date_input)

Â  Â  Â  Â Â 

Â  Â  Â  Â  if tanggal_awal and tanggal_akhir:

Â  Â  Â  Â  Â  Â  start_time = time.time()

Â  Â  Â  Â  Â  Â  df_lapus_untuk_proses = df_lapus[kategori_terpilih] if mode_kategori == 'Pilih Kategori Tertentu' else df_lapus

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  st.markdown("---")

Â  Â  Â  Â  Â  Â  st.header("Proses & Hasil Scraping")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  keyword_placeholder = st.empty()

Â  Â  Â  Â  Â  Â  table_placeholder = st.empty()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  with table_placeholder.container():

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("### Hasil Scraping Terkini")

Â  Â  Â  Â  Â  Â  Â  Â  st.info("Menunggu hasil pertama ditemukan...")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  hasil_df = start_scraping(tanggal_awal, tanggal_akhir, df_lapus_untuk_proses, df_daerah, start_time, table_placeholder, keyword_placeholder)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  end_time = time.time()

Â  Â  Â  Â  Â  Â  total_duration_str = f"{int((end_time - start_time) // 60)} menit {int((end_time - start_time) % 60)} detik"



Â  Â  Â  Â  Â  Â  st.header("âœ… Proses Selesai")

Â  Â  Â  Â  Â  Â  st.success(f"Scraping telah selesai dalam {total_duration_str}.")



Â  Â  Â  Â  Â  Â  if not hasil_df.empty:

Â  Â  Â  Â  Â  Â  Â  Â  output = io.BytesIO()

Â  Â  Â  Â  Â  Â  Â  Â  with pd.ExcelWriter(output, engine='openpyxl') as writer:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hasil_df.to_excel(writer, sheet_name="Hasil Scraping", index=False)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label="ğŸ“¥ Unduh Hasil Scraping (Excel)",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data=output.getvalue(),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name=f"Hasil_Scraping_{time.strftime('%Y%m%d-%H%M%S')}.xlsx",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  use_container_width=True

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Tidak ada berita yang ditemukan sesuai dengan parameter dan kata kunci yang Anda pilih.")



Â  Â  Â  Â  Â  Â  if st.button("ğŸ”„ Mulai Scraping Baru (Reset)", use_container_width=True):

Â  Â  Â  Â  Â  Â  Â  Â  if 'hasil_scraping' in st.session_state:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  del st.session_state.hasil_scraping

Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  st.error("Rentang tanggal tidak valid. Silakan periksa kembali pilihan Anda.")



# --- NAVIGASI DAN LOGIKA UTAMA ---



if "page" not in st.session_state:

Â  Â  st.session_state.page = "Home"

if "logged_in" not in st.session_state:

Â  Â  st.session_state.logged_in = False

if "sub_page" not in st.session_state:

Â  Â  st.session_state.sub_page = "Neraca"



with st.sidebar:

Â  Â  st.image("logo bps konsel.png")

Â  Â Â 

Â  Â  if not st.session_state.logged_in:

Â  Â  Â  Â  with st.form("login_form"):

Â  Â  Â  Â  Â  Â  st.header("Login")

Â  Â  Â  Â  Â  Â  username = st.text_input("Username")

Â  Â  Â  Â  Â  Â  password = st.text_input("Password", type="password")

Â  Â  Â  Â  Â  Â  if st.form_submit_button("Login", use_container_width=True, type="primary"):

Â  Â  Â  Â  Â  Â  Â  Â  if username == "user7405" and password == "bps7405":

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.logged_in = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.page = "Home"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Username atau password salah. Hubungi admin untuk bantuan.")

Â  Â  else:

Â  Â  Â  Â  st.success(f"Selamat datang, **user7405**!")

Â  Â  Â  Â  if st.button("Logout", use_container_width=True):

Â  Â  Â  Â  Â  Â  st.session_state.logged_in = False

Â  Â  Â  Â  Â  Â  st.session_state.page = "Home"

Â  Â  Â  Â  Â  Â  st.rerun()



Â  Â  st.markdown("---")

Â  Â  st.header("Menu Navigasi")

Â  Â Â 

Â  Â  if st.button("ğŸ  Home", use_container_width=True):

Â  Â  Â  Â  set_page("Home"); st.rerun()

Â  Â  Â  Â Â 

Â  Â  if st.button("ğŸ“– Pendahuluan", use_container_width=True):

Â  Â  Â  Â  set_page("Pendahuluan"); st.rerun()



Â  Â  if st.session_state.logged_in:

Â  Â  Â  Â  if st.button("âš™ï¸ Scraping", use_container_width=True):

Â  Â  Â  Â  Â  Â  set_page("Scraping"); st.rerun()

Â  Â  Â  Â Â 

Â  Â  Â  Â  if st.button("ğŸ—‚ï¸ Dokumentasi", use_container_width=True):

Â  Â  Â  Â  Â  Â  set_page("Dokumentasi"); st.rerun()



if st.session_state.page == "Home":

Â  Â  show_home_page()

elif st.session_state.page == "Pendahuluan":

Â  Â  show_pendahuluan_page()

elif st.session_state.page == "Scraping" and st.session_state.logged_in:

Â  Â  show_scraping_page()

elif st.session_state.page == "Dokumentasi" and st.session_state.logged_in:

Â  Â  show_documentation_page()

else:

Â  Â  st.session_state.page = "Home"

Â  Â  st.rerun()
